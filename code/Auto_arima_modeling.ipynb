{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import plot_mpl\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "for i in range(1,3):\n",
    "    data = pd.read_csv(\"../data/funda_train_%d주_수정.csv\"%(i))\n",
    "    data[\"Date\"] = data[\"Date\"]=pd.to_datetime(data[\"Date\"], format=\"%Y-%m-%d\")\n",
    "    datas.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기간별 데이터셋 리딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datas[0][\"store_id\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompostion_plot(sample,dat,id_col,int_col,date_col):\n",
    "    data = dat.copy()\n",
    "    data.index = data[date_col]\n",
    "    for i in sample.tolist():\n",
    "        try:\n",
    "            result = seasonal_decompose(np.log1p(data[data[id_col]==i][int_col]))\n",
    "            fig = result.plot()\n",
    "            plot_mpl(fig)\n",
    "        except:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeeWonSeok\\Anaconda3\\envs\\django\\lib\\site-packages\\plotly\\matplotlylib\\mpltools.py:368: MatplotlibDeprecationWarning:\n",
      "\n",
      "\n",
      "The is_frame_like function was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "\n",
      "C:\\Users\\LeeWonSeok\\Anaconda3\\envs\\django\\lib\\site-packages\\plotly\\matplotlylib\\mpltools.py:368: MatplotlibDeprecationWarning:\n",
      "\n",
      "\n",
      "The is_frame_like function was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "\n",
      "C:\\Users\\LeeWonSeok\\Anaconda3\\envs\\django\\lib\\site-packages\\plotly\\matplotlylib\\mpltools.py:368: MatplotlibDeprecationWarning:\n",
      "\n",
      "\n",
      "The is_frame_like function was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "\n",
      "C:\\Users\\LeeWonSeok\\Anaconda3\\envs\\django\\lib\\site-packages\\plotly\\matplotlylib\\mpltools.py:368: MatplotlibDeprecationWarning:\n",
      "\n",
      "\n",
      "The is_frame_like function was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decompostion_plot(sample,datas[0],\"store_id\",\"amount\",\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto arima 모형 소개\n",
    "- 최적 parmater를 자동으로 찾아서 arima모델을 fitting 해줌\n",
    "- 기본적으로 p(AR),d(차분),q(MR) 파라미터를 튜닝\n",
    "- 계절성분 추가시 SARMIA의 P,Q,R 도 튜닝할 수 있음\n",
    "- AIC,AICC,BIC,HQIC,OOB 등을 고려하여 다음 값이 가장 작은 모델을 리턴함\n",
    "- oob 는 Out of bag 방식\n",
    "## Parameter\n",
    "- y : 예측할 값(nan,inf 혀용 x)\n",
    "- exogenous : y값이외의 추가 데이터 사용 예측(2차원 array 형태로 input)\n",
    "- information_criterion : aic,bic,oob 등 피팅기준\n",
    "- start_p : ARIMA(p,d,q)의 grid_search p 시작값\n",
    "- start_q : ARIMA(p,d,q)의 grid_search q 시작값\n",
    "- max_p : ARIMA(p,d,q)의 grid_search p 최대값\n",
    "- max_q : ARIMA(p,d,q)의 grid_search q 최대값\n",
    "- d : ARIMA(p,d,q)의 차분성분(None이면 알아서 하지만 long time 보통 1,2 안에)\n",
    "- start_P : SARIMA(P,D,Q)의 grid_search p 시작값\n",
    "- start_Q : SARIMA(P,D,v)의 grid_search q 시작값\n",
    "- max_P : SARIMA(P,D,Q)의 grid_search p 최대값\n",
    "- max_Q : SARIMA(P,D,Q)의 grid_search q 최대값\n",
    "- D : SARIMA(p,D,q)의 차분성분(None이면 알아서 하지만 long time 보통 1,2 안에)\n",
    "- max_order : ARIMA(p,d,q)의 p와 q의 합 최대를 제한 (default 5, None이면 제약없음)\n",
    "- m : season parameter , 계절을 얼마나 할건지(ex 4계절 4, 월별 12 등 상황따라)\n",
    "- seasonal : SARIMA 할건지 정하는것  m==1 일 땐 알아서 False임\n",
    "- stationary : 정상시계열인지 아닌지 True일 때 d==0 이어야함\n",
    "- stepwise : 단계적선택법을 적용해 paramter 찾을지 여부 (True 추천)\n",
    "- n_jobs : 지정한 수 범위내에서 가능한 arima model 다 fitting함\n",
    "- trend : 추세다항식성분 넣을건지 \"c\" 상수, \"t\" 1차항하나 , \"ct\" 상수랑 1차항둘다\n",
    "- supress_warnings : arima 모델추정시 발생할수있는  warning 무시할건지 \n",
    "- error_action : 피팅도중 error 발생시 행동 (\"ignore\",\"warn\")\n",
    "- trace : 피팅되는과정  print할지말지  \n",
    "- random : 랜덤 서칭으로 parameter찾음 (단계적이 false일때 씀)\n",
    "- n_fits : 랜덤 서칭으로 몇번 iteration할건지\n",
    "- out_of_sample_size : 피팅할때 데이터꼬리 일부를 빼놓는 방식(검증용) , self.arima_res_.data.endog values에 있음\n",
    "- scoring : {mae,mse} 기준으로 out_of_sample_size>0일 때 검증함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arima parmeter tunning plan\n",
    "1. ARIMA(0,0,0) ~ ARIMA(10,2,10) 까지 \n",
    "2. SARIMA(0,0,0) ~ SARIMA(5,2,5) 까지\n",
    "3. stepwise = True\n",
    "4. seasonal = {True, False}\n",
    "5. m = {half:2,season:4,month:12}\n",
    "6. out_sample => {1주 :4, 2주:2, 3주:2, 4주:1}\n",
    "7. scoring = mae\n",
    "8. supress_warnings = True\n",
    "9. error_action = \"ignore\"\n",
    "10. trend = [\"c\",\"t\",\"ct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save complete\n"
     ]
    }
   ],
   "source": [
    "# 주차별 결과 테이블로 나타내기\n",
    "# 모델링 경과 MAE가 가장 좋은 모델 쓰기\n",
    "# 피팅된 모델 결과도 기록하기\n",
    "total_mean_result = pd.DataFrame(columns=[\"unit\",\"mae\",\"rmse\",\"mae_log\",\"rmse_log\"]) \n",
    "for unit,data in enumerate(datas,1):\n",
    "    total_result = pd.DataFrame(columns=[\"store_id\",\"mae\",\"rmse\",\"mae_log\",\"rmse_log\"])\n",
    "    store_id_lst = data[\"store_id\"].unique()\n",
    "    for ids in store_id_lst:\n",
    "        # id filtering and train test split\n",
    "        each_store = data[data[\"store_id\"]==ids]\n",
    "        store_train = each_store[each_store[\"Date\"]<'2019-01-01']\n",
    "        store_test = each_store[each_store[\"Date\"]>='2019-01-01']\n",
    "        test_period = len(store_test)\n",
    "        if test_period == 0:\n",
    "            continue\n",
    "        # gird search modeling\n",
    "        modeling = time_model(np.log1p(store_train[\"amount\"].values))\n",
    "        result_model = modeling.auto_arimga_grid()\n",
    "        MAE_result = []\n",
    "        y_true = store_test[\"amount\"].values\n",
    "        y_true_log = np.log1p(y_true)\n",
    "        # best model select\n",
    "        for each_model in result_model:\n",
    "            pred = each_model.predict(n_periods = test_period)\n",
    "            pred = np.expm1(pred)\n",
    "            mae = modeling.MAE(y_true,pred)\n",
    "            MAE_result.append(mae)\n",
    "        best_model = result_model[np.argmin(MAE_result)]\n",
    "        # calculate metrics\n",
    "        best_mae = np.min(MAE_result)\n",
    "        y_pred_log = best_model.predict(n_periods = test_period)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        best_rmse = modeling.RMSE(y_true,y_pred)\n",
    "        mae_log = modeling.MAE(y_true_log,y_pred_log)\n",
    "        rmse_log = modeling.RMSE(y_true_log,y_pred_log)\n",
    "        # save_best_model\n",
    "        modeling.save_model(best_model,'../model/model_%d_%d.sav'%(unit,ids))\n",
    "        # append reulst to DataFrame\n",
    "        mini_result = pd.DataFrame.from_dict([{\"store_id\" :ids,\n",
    "                                    \"mae\":best_mae,\n",
    "                                    \"rmse\":best_rmse,\n",
    "                                     \"mae_log\":mae_log,\n",
    "                                      \"rmse_log\":rmse_log}])\n",
    "        total_result = pd.concat([total_result,mini_result],axis=0,sort=False)\n",
    "    # caulcate mean result\n",
    "    mean_mae = total_result[\"mae\"].mean()\n",
    "    mean_rmse = total_result[\"rmse\"].mean()\n",
    "    mean_mae_log = total_result[\"mae_log\"].mean()\n",
    "    mean_rmse_log = total_result[\"rmse_log\"].mean()\n",
    "    mini_mean_result = pd.DataFrame.from_dict([{\"unit\":unit,\n",
    "                                    \"mae\":mean_mae,\n",
    "                                    \"rmse\":mean_rmse,\n",
    "                                    \"mae_log\":mean_mae_log,\n",
    "                                    \"rmse_log\":mean_rmse_log}])\n",
    "    total_mean_result = pd.concat([total_mean_result,mini_mean_result],axis=0,sort=False)\n",
    "    # result to excel\n",
    "    total_result.to_excel(\"../result/each_store_result_%d.xlsx\"%(unit))\n",
    "total_mean_result.to_excel('../result/total_unit_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
